{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "# Import TensorFlow >= 1.10 and enable eager execution\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Malaga\n"
     ]
    }
   ],
   "source": [
    "import unidecode\n",
    "filepath = \"/Users/pallavitaneja/Downloads/fra-eng/fra.txt\"\n",
    "accented_string = u'Málaga'\n",
    "# accented_string is of type 'unicode'\n",
    "unaccented_string = unidecode.unidecode(accented_string)\n",
    "# unaccented_string contains 'Malaga'and is of type 'str'\n",
    "print(unaccented_string)\n",
    "lines = open(filepath, encoding='UTF-8').read().strip().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The multiplication produces::: tf.Tensor(12, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "at = tf.constant(3) \n",
    "bt = tf.constant(4)\n",
    "mult = tf.multiply(at, bt)\n",
    "print (\"The multiplication produces:::\", mult)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "tf.placeholder() is not compatible with eager execution.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-d8bcac9c7377>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"p\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/newenv/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mplaceholder\u001b[0;34m(dtype, shape, name)\u001b[0m\n\u001b[1;32m   1742\u001b[0m   \"\"\"\n\u001b[1;32m   1743\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1744\u001b[0;31m     raise RuntimeError(\"tf.placeholder() is not compatible with \"\n\u001b[0m\u001b[1;32m   1745\u001b[0m                        \"eager execution.\")\n\u001b[1;32m   1746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: tf.placeholder() is not compatible with eager execution."
     ]
    }
   ],
   "source": [
    "pl = tf.placeholder(tf.float32, name=\"p\") \n",
    "pi = tf.constant(3.) \n",
    "c = tf.add(pl, pi)\n",
    "print (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the unicode file to ascii\n",
    "import unidecode\n",
    "filepath = \"/Users/pallavitaneja/Downloads/fra-eng/fra.txt\"\n",
    "def sent_preprocessing(w):\n",
    "    start_tag = '<SOS>'\n",
    "    end_tag = '<EOS>'\n",
    "    w = unidecode.unidecode(w.lower().strip())\n",
    "    #Regex for padding punctuation with white spaces\n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "    #print(w)\n",
    "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "    # For Example: \n",
    "    #i'm ok .  i m ok .\n",
    "    #j'ai perdu. j ai perdu .\n",
    "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "    #print(\"w \"+ w)\n",
    "    w = w.rstrip().strip()\n",
    "    # adding a start and an end token to the sentence to indicate to start and stop predicting.\n",
    "    w = start_tag + ' ' + w + ' ' + end_tag\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Remove the accents\n",
    "# 2. Clean the sentences\n",
    "# 3. Return word pairs in the format: [FRENCH, ENGLISH]\n",
    "def generate_sent_pairs(filepath, num_examples):\n",
    "    lines = open(filepath, encoding='UTF-8').read().strip().split('\\n')\n",
    "    sent_pairs = []\n",
    "    for line in lines[:num_examples]:\n",
    "        l = []\n",
    "        for sent in line.split('\\t'):\n",
    "            l.append(sent_preprocessing(sent))\n",
    "        sent_pairs.append(l)\n",
    "#     print(sent_pairs)\n",
    "    return sent_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Remove the accents\n",
    "# 2. Clean the sentences\n",
    "# 3. Return word pairs in the format: [ENGLISH, SPANISH]\n",
    "def create_dataset(filepath, num_examples):\n",
    "    lines = open(filepath, encoding='UTF-8').read().strip().split('\\n')\n",
    "    \n",
    "    word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
    "    \n",
    "    return word_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageIndex():\n",
    "    def __init__(self, lang):\n",
    "        self.lang = lang\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.vocab = set()\n",
    "    \n",
    "        self.create_index()\n",
    "    \n",
    "    def create_index(self):\n",
    "        for phrase in self.lang:\n",
    "            self.vocab.update(phrase.split(' '))\n",
    "        self.vocab = sorted(self.vocab)\n",
    "        self.word2idx['<pad>'] = 0\n",
    "        for index, word in enumerate(self.vocab):\n",
    "            self.word2idx[word] = index + 1\n",
    "        for word, index in self.word2idx.items():\n",
    "            self.idx2word[index] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go.\n",
      "go.\n",
      "go.\n",
      "Va !\n",
      "va !\n",
      "va !\n",
      "Hi.\n",
      "hi.\n",
      "hi.\n",
      "Salut !\n",
      "salut !\n",
      "salut !\n",
      "Run!\n",
      "run!\n",
      "run!\n",
      "Cours !\n",
      "cours !\n",
      "cours !\n",
      "Run!\n",
      "run!\n",
      "run!\n",
      "Courez !\n",
      "courez !\n",
      "courez !\n",
      "Wow!\n",
      "wow!\n",
      "wow!\n",
      "Ça alors !\n",
      "ca alors !\n",
      "ca alors !\n",
      "Fire!\n",
      "fire!\n",
      "fire!\n",
      "Au feu !\n",
      "au feu !\n",
      "au feu !\n",
      "Help!\n",
      "help!\n",
      "help!\n",
      "À l'aide !\n",
      "a l'aide !\n",
      "a l'aide !\n",
      "Jump.\n",
      "jump.\n",
      "jump.\n",
      "Saute.\n",
      "saute.\n",
      "saute.\n",
      "Stop!\n",
      "stop!\n",
      "stop!\n",
      "Ça suffit !\n",
      "ca suffit !\n",
      "ca suffit !\n",
      "Stop!\n",
      "stop!\n",
      "stop!\n",
      "Stop !\n",
      "stop !\n",
      "stop !\n",
      "Stop!\n",
      "stop!\n",
      "stop!\n",
      "Arrête-toi !\n",
      "arrete-toi !\n",
      "arrete-toi !\n",
      "Wait!\n",
      "wait!\n",
      "wait!\n",
      "Attends !\n",
      "attends !\n",
      "attends !\n",
      "Wait!\n",
      "wait!\n",
      "wait!\n",
      "Attendez !\n",
      "attendez !\n",
      "attendez !\n",
      "Go on.\n",
      "go on.\n",
      "go on.\n",
      "Poursuis.\n",
      "poursuis.\n",
      "poursuis.\n",
      "Go on.\n",
      "go on.\n",
      "go on.\n",
      "Continuez.\n",
      "continuez.\n",
      "continuez.\n",
      "Go on.\n",
      "go on.\n",
      "go on.\n",
      "Poursuivez.\n",
      "poursuivez.\n",
      "poursuivez.\n",
      "Hello!\n",
      "hello!\n",
      "hello!\n",
      "Bonjour !\n",
      "bonjour !\n",
      "bonjour !\n",
      "Hello!\n",
      "hello!\n",
      "hello!\n",
      "Salut !\n",
      "salut !\n",
      "salut !\n",
      "I see.\n",
      "i see.\n",
      "i see.\n",
      "Je comprends.\n",
      "je comprends.\n",
      "je comprends.\n",
      "I try.\n",
      "i try.\n",
      "i try.\n",
      "J'essaye.\n",
      "j'essaye.\n",
      "j'essaye.\n",
      "I won!\n",
      "i won!\n",
      "i won!\n",
      "J'ai gagné !\n",
      "j'ai gagne !\n",
      "j'ai gagne !\n",
      "I won!\n",
      "i won!\n",
      "i won!\n",
      "Je l'ai emporté !\n",
      "je l'ai emporte !\n",
      "je l'ai emporte !\n",
      "Oh no!\n",
      "oh no!\n",
      "oh no!\n",
      "Oh non !\n",
      "oh non !\n",
      "oh non !\n",
      "Attack!\n",
      "attack!\n",
      "attack!\n",
      "Attaque !\n",
      "attaque !\n",
      "attaque !\n",
      "Attack!\n",
      "attack!\n",
      "attack!\n",
      "Attaquez !\n",
      "attaquez !\n",
      "attaquez !\n",
      "Cheers!\n",
      "cheers!\n",
      "cheers!\n",
      "Santé !\n",
      "sante !\n",
      "sante !\n",
      "Cheers!\n",
      "cheers!\n",
      "cheers!\n",
      "À votre santé !\n",
      "a votre sante !\n",
      "a votre sante !\n",
      "Cheers!\n",
      "cheers!\n",
      "cheers!\n",
      "Merci !\n",
      "merci !\n",
      "merci !\n",
      "Cheers!\n",
      "cheers!\n",
      "cheers!\n",
      "Tchin-tchin !\n",
      "tchin-tchin !\n",
      "tchin-tchin !\n",
      "Get up.\n",
      "get up.\n",
      "get up.\n",
      "Lève-toi.\n",
      "leve-toi.\n",
      "leve-toi.\n",
      "Go now.\n",
      "go now.\n",
      "go now.\n",
      "Va, maintenant.\n",
      "va, maintenant.\n",
      "va, maintenant.\n",
      "Go now.\n",
      "go now.\n",
      "go now.\n",
      "Allez-y maintenant.\n",
      "allez-y maintenant.\n",
      "allez-y maintenant.\n",
      "Go now.\n",
      "go now.\n",
      "go now.\n",
      "Vas-y maintenant.\n",
      "vas-y maintenant.\n",
      "vas-y maintenant.\n",
      "Got it!\n",
      "got it!\n",
      "got it!\n",
      "J'ai pigé !\n",
      "j'ai pige !\n",
      "j'ai pige !\n",
      "Got it!\n",
      "got it!\n",
      "got it!\n",
      "Compris !\n",
      "compris !\n",
      "compris !\n",
      "Got it?\n",
      "got it?\n",
      "got it?\n",
      "Pigé ?\n",
      "pige ?\n",
      "pige ?\n",
      "Got it?\n",
      "got it?\n",
      "got it?\n",
      "Compris ?\n",
      "compris ?\n",
      "compris ?\n",
      "Got it?\n",
      "got it?\n",
      "got it?\n",
      "T'as capté ?\n",
      "t'as capte ?\n",
      "t'as capte ?\n",
      "Hop in.\n",
      "hop in.\n",
      "hop in.\n",
      "Monte.\n",
      "monte.\n",
      "monte.\n",
      "Hop in.\n",
      "hop in.\n",
      "hop in.\n",
      "Montez.\n",
      "montez.\n",
      "montez.\n",
      "Hug me.\n",
      "hug me.\n",
      "hug me.\n",
      "Serre-moi dans tes bras !\n",
      "serre-moi dans tes bras !\n",
      "serre-moi dans tes bras !\n",
      "Hug me.\n",
      "hug me.\n",
      "hug me.\n",
      "Serrez-moi dans vos bras !\n",
      "serrez-moi dans vos bras !\n",
      "serrez-moi dans vos bras !\n",
      "I fell.\n",
      "i fell.\n",
      "i fell.\n",
      "Je suis tombée.\n",
      "je suis tombee.\n",
      "je suis tombee.\n",
      "I fell.\n",
      "i fell.\n",
      "i fell.\n",
      "Je suis tombé.\n",
      "je suis tombe.\n",
      "je suis tombe.\n",
      "I know.\n",
      "i know.\n",
      "i know.\n",
      "Je sais.\n",
      "je sais.\n",
      "je sais.\n",
      "I left.\n",
      "i left.\n",
      "i left.\n",
      "Je suis parti.\n",
      "je suis parti.\n",
      "je suis parti.\n",
      "I left.\n",
      "i left.\n",
      "i left.\n",
      "Je suis partie.\n",
      "je suis partie.\n",
      "je suis partie.\n",
      "I lost.\n",
      "i lost.\n",
      "i lost.\n",
      "J'ai perdu.\n",
      "j'ai perdu.\n",
      "j'ai perdu.\n",
      "I'm 19.\n",
      "i'm 19.\n",
      "i'm 19.\n",
      "J'ai 19 ans.\n",
      "j'ai 19 ans.\n",
      "j'ai 19 ans.\n",
      "I'm OK.\n",
      "i'm ok.\n",
      "i'm ok.\n",
      "Je vais bien.\n",
      "je vais bien.\n",
      "je vais bien.\n",
      "I'm OK.\n",
      "i'm ok.\n",
      "i'm ok.\n",
      "Ça va.\n",
      "ca va.\n",
      "ca va.\n",
      "Listen.\n",
      "listen.\n",
      "listen.\n",
      "Écoutez !\n",
      "ecoutez !\n",
      "ecoutez !\n",
      "No way!\n",
      "no way!\n",
      "no way!\n",
      "C'est pas possible !\n",
      "c'est pas possible !\n",
      "c'est pas possible !\n",
      "No way!\n",
      "no way!\n",
      "no way!\n",
      "Impossible !\n",
      "impossible !\n",
      "impossible !\n",
      "No way!\n",
      "no way!\n",
      "no way!\n",
      "En aucun cas.\n",
      "en aucun cas.\n",
      "en aucun cas.\n",
      "No way!\n",
      "no way!\n",
      "no way!\n",
      "Sans façons !\n",
      "sans facons !\n",
      "sans facons !\n",
      "No way!\n",
      "no way!\n",
      "no way!\n",
      "C'est hors de question !\n",
      "c'est hors de question !\n",
      "c'est hors de question !\n",
      "No way!\n",
      "no way!\n",
      "no way!\n",
      "Il n'en est pas question !\n",
      "il n'en est pas question !\n",
      "il n'en est pas question !\n",
      "No way!\n",
      "no way!\n",
      "no way!\n",
      "C'est exclu !\n",
      "c'est exclu !\n",
      "c'est exclu !\n",
      "No way!\n",
      "no way!\n",
      "no way!\n",
      "En aucune manière !\n",
      "en aucune maniere !\n",
      "en aucune maniere !\n",
      "No way!\n",
      "no way!\n",
      "no way!\n",
      "Hors de question !\n",
      "hors de question !\n",
      "hors de question !\n",
      "Really?\n",
      "really?\n",
      "really?\n",
      "Vraiment ?\n",
      "vraiment ?\n",
      "vraiment ?\n",
      "Really?\n",
      "really?\n",
      "really?\n",
      "Vrai ?\n",
      "vrai ?\n",
      "vrai ?\n",
      "Really?\n",
      "really?\n",
      "really?\n",
      "Ah bon ?\n",
      "ah bon ?\n",
      "ah bon ?\n",
      "Thanks.\n",
      "thanks.\n",
      "thanks.\n",
      "Merci !\n",
      "merci !\n",
      "merci !\n",
      "We try.\n",
      "we try.\n",
      "we try.\n",
      "On essaye.\n",
      "on essaye.\n",
      "on essaye.\n",
      "We won.\n",
      "we won.\n",
      "we won.\n",
      "Nous avons gagné.\n",
      "nous avons gagne.\n",
      "nous avons gagne.\n",
      "We won.\n",
      "we won.\n",
      "we won.\n",
      "Nous gagnâmes.\n",
      "nous gagnames.\n",
      "nous gagnames.\n",
      "We won.\n",
      "we won.\n",
      "we won.\n",
      "Nous l'avons emporté.\n",
      "nous l'avons emporte.\n",
      "nous l'avons emporte.\n",
      "We won.\n",
      "we won.\n",
      "we won.\n",
      "Nous l'emportâmes.\n",
      "nous l'emportames.\n",
      "nous l'emportames.\n",
      "Ask Tom.\n",
      "ask tom.\n",
      "ask tom.\n",
      "Demande à Tom.\n",
      "demande a tom.\n",
      "demande a tom.\n",
      "Awesome!\n",
      "awesome!\n",
      "awesome!\n",
      "Fantastique !\n",
      "fantastique !\n",
      "fantastique !\n",
      "Be calm.\n",
      "be calm.\n",
      "be calm.\n",
      "Sois calme !\n",
      "sois calme !\n",
      "sois calme !\n",
      "Be calm.\n",
      "be calm.\n",
      "be calm.\n",
      "Soyez calme !\n",
      "soyez calme !\n",
      "soyez calme !\n",
      "Be calm.\n",
      "be calm.\n",
      "be calm.\n",
      "Soyez calmes !\n",
      "soyez calmes !\n",
      "soyez calmes !\n",
      "Be cool.\n",
      "be cool.\n",
      "be cool.\n",
      "Sois détendu !\n",
      "sois detendu !\n",
      "sois detendu !\n",
      "Be fair.\n",
      "be fair.\n",
      "be fair.\n",
      "Sois juste !\n",
      "sois juste !\n",
      "sois juste !\n",
      "Be fair.\n",
      "be fair.\n",
      "be fair.\n",
      "Soyez juste !\n",
      "soyez juste !\n",
      "soyez juste !\n",
      "Be fair.\n",
      "be fair.\n",
      "be fair.\n",
      "Soyez justes !\n",
      "soyez justes !\n",
      "soyez justes !\n",
      "Be fair.\n",
      "be fair.\n",
      "be fair.\n",
      "Sois équitable !\n",
      "sois equitable !\n",
      "sois equitable !\n",
      "Be fair.\n",
      "be fair.\n",
      "be fair.\n",
      "Soyez équitable !\n",
      "soyez equitable !\n",
      "soyez equitable !\n",
      "Be fair.\n",
      "be fair.\n",
      "be fair.\n",
      "Soyez équitables !\n",
      "soyez equitables !\n",
      "soyez equitables !\n",
      "Be kind.\n",
      "be kind.\n",
      "be kind.\n",
      "Sois gentil.\n",
      "sois gentil.\n",
      "sois gentil.\n",
      "Be nice.\n",
      "be nice.\n",
      "be nice.\n",
      "Sois gentil !\n",
      "sois gentil !\n",
      "sois gentil !\n",
      "Be nice.\n",
      "be nice.\n",
      "be nice.\n",
      "Sois gentille !\n",
      "sois gentille !\n",
      "sois gentille !\n",
      "Be nice.\n",
      "be nice.\n",
      "be nice.\n",
      "Soyez gentil !\n",
      "soyez gentil !\n",
      "soyez gentil !\n",
      "Be nice.\n",
      "be nice.\n",
      "be nice.\n",
      "Soyez gentille !\n",
      "soyez gentille !\n",
      "soyez gentille !\n",
      "Be nice.\n",
      "be nice.\n",
      "be nice.\n",
      "Soyez gentils !\n",
      "soyez gentils !\n",
      "soyez gentils !\n",
      "Be nice.\n",
      "be nice.\n",
      "be nice.\n",
      "Soyez gentilles !\n",
      "soyez gentilles !\n",
      "soyez gentilles !\n",
      "Beat it.\n",
      "beat it.\n",
      "beat it.\n",
      "Dégage !\n",
      "degage !\n",
      "degage !\n",
      "Call me.\n",
      "call me.\n",
      "call me.\n",
      "Appelle-moi !\n",
      "appelle-moi !\n",
      "appelle-moi !\n",
      "Call me.\n",
      "call me.\n",
      "call me.\n",
      "Appellez-moi !\n",
      "appellez-moi !\n",
      "appellez-moi !\n",
      "Call us.\n",
      "call us.\n",
      "call us.\n",
      "Appelle-nous !\n",
      "appelle-nous !\n",
      "appelle-nous !\n",
      "Call us.\n",
      "call us.\n",
      "call us.\n",
      "Appelez-nous !\n",
      "appelez-nous !\n",
      "appelez-nous !\n",
      "Come in.\n",
      "come in.\n",
      "come in.\n",
      "Entrez !\n",
      "entrez !\n",
      "entrez !\n",
      "Come in.\n",
      "come in.\n",
      "come in.\n",
      "Entre.\n",
      "entre.\n",
      "entre.\n",
      "Come in.\n",
      "come in.\n",
      "come in.\n",
      "Entre !\n",
      "entre !\n",
      "entre !\n",
      "Come in.\n",
      "come in.\n",
      "come in.\n",
      "Entrez !\n",
      "entrez !\n",
      "entrez !\n",
      "Come on!\n",
      "come on!\n",
      "come on!\n",
      "Allez !\n",
      "allez !\n",
      "allez !\n",
      "Come on.\n",
      "come on.\n",
      "come on.\n",
      "Allez !\n",
      "allez !\n",
      "allez !\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['<start> go . <end>', '<start> va ! <end>'],\n",
       " ['<start> hi . <end>', '<start> salut ! <end>'],\n",
       " ['<start> run ! <end>', '<start> cours ! <end>'],\n",
       " ['<start> run ! <end>', '<start> courez ! <end>'],\n",
       " ['<start> wow ! <end>', '<start> ca alors ! <end>'],\n",
       " ['<start> fire ! <end>', '<start> au feu ! <end>'],\n",
       " ['<start> help ! <end>', '<start> a l aide ! <end>'],\n",
       " ['<start> jump . <end>', '<start> saute . <end>'],\n",
       " ['<start> stop ! <end>', '<start> ca suffit ! <end>'],\n",
       " ['<start> stop ! <end>', '<start> stop ! <end>'],\n",
       " ['<start> stop ! <end>', '<start> arrete toi ! <end>'],\n",
       " ['<start> wait ! <end>', '<start> attends ! <end>'],\n",
       " ['<start> wait ! <end>', '<start> attendez ! <end>'],\n",
       " ['<start> go on . <end>', '<start> poursuis . <end>'],\n",
       " ['<start> go on . <end>', '<start> continuez . <end>'],\n",
       " ['<start> go on . <end>', '<start> poursuivez . <end>'],\n",
       " ['<start> hello ! <end>', '<start> bonjour ! <end>'],\n",
       " ['<start> hello ! <end>', '<start> salut ! <end>'],\n",
       " ['<start> i see . <end>', '<start> je comprends . <end>'],\n",
       " ['<start> i try . <end>', '<start> j essaye . <end>'],\n",
       " ['<start> i won ! <end>', '<start> j ai gagne ! <end>'],\n",
       " ['<start> i won ! <end>', '<start> je l ai emporte ! <end>'],\n",
       " ['<start> oh no ! <end>', '<start> oh non ! <end>'],\n",
       " ['<start> attack ! <end>', '<start> attaque ! <end>'],\n",
       " ['<start> attack ! <end>', '<start> attaquez ! <end>'],\n",
       " ['<start> cheers ! <end>', '<start> sante ! <end>'],\n",
       " ['<start> cheers ! <end>', '<start> a votre sante ! <end>'],\n",
       " ['<start> cheers ! <end>', '<start> merci ! <end>'],\n",
       " ['<start> cheers ! <end>', '<start> tchin tchin ! <end>'],\n",
       " ['<start> get up . <end>', '<start> leve toi . <end>'],\n",
       " ['<start> go now . <end>', '<start> va , maintenant . <end>'],\n",
       " ['<start> go now . <end>', '<start> allez y maintenant . <end>'],\n",
       " ['<start> go now . <end>', '<start> vas y maintenant . <end>'],\n",
       " ['<start> got it ! <end>', '<start> j ai pige ! <end>'],\n",
       " ['<start> got it ! <end>', '<start> compris ! <end>'],\n",
       " ['<start> got it ? <end>', '<start> pige ? <end>'],\n",
       " ['<start> got it ? <end>', '<start> compris ? <end>'],\n",
       " ['<start> got it ? <end>', '<start> t as capte ? <end>'],\n",
       " ['<start> hop in . <end>', '<start> monte . <end>'],\n",
       " ['<start> hop in . <end>', '<start> montez . <end>'],\n",
       " ['<start> hug me . <end>', '<start> serre moi dans tes bras ! <end>'],\n",
       " ['<start> hug me . <end>', '<start> serrez moi dans vos bras ! <end>'],\n",
       " ['<start> i fell . <end>', '<start> je suis tombee . <end>'],\n",
       " ['<start> i fell . <end>', '<start> je suis tombe . <end>'],\n",
       " ['<start> i know . <end>', '<start> je sais . <end>'],\n",
       " ['<start> i left . <end>', '<start> je suis parti . <end>'],\n",
       " ['<start> i left . <end>', '<start> je suis partie . <end>'],\n",
       " ['<start> i lost . <end>', '<start> j ai perdu . <end>'],\n",
       " ['<start> i m . <end>', '<start> j ai ans . <end>'],\n",
       " ['<start> i m ok . <end>', '<start> je vais bien . <end>'],\n",
       " ['<start> i m ok . <end>', '<start> ca va . <end>'],\n",
       " ['<start> listen . <end>', '<start> ecoutez ! <end>'],\n",
       " ['<start> no way ! <end>', '<start> c est pas possible ! <end>'],\n",
       " ['<start> no way ! <end>', '<start> impossible ! <end>'],\n",
       " ['<start> no way ! <end>', '<start> en aucun cas . <end>'],\n",
       " ['<start> no way ! <end>', '<start> sans facons ! <end>'],\n",
       " ['<start> no way ! <end>', '<start> c est hors de question ! <end>'],\n",
       " ['<start> no way ! <end>', '<start> il n en est pas question ! <end>'],\n",
       " ['<start> no way ! <end>', '<start> c est exclu ! <end>'],\n",
       " ['<start> no way ! <end>', '<start> en aucune maniere ! <end>'],\n",
       " ['<start> no way ! <end>', '<start> hors de question ! <end>'],\n",
       " ['<start> really ? <end>', '<start> vraiment ? <end>'],\n",
       " ['<start> really ? <end>', '<start> vrai ? <end>'],\n",
       " ['<start> really ? <end>', '<start> ah bon ? <end>'],\n",
       " ['<start> thanks . <end>', '<start> merci ! <end>'],\n",
       " ['<start> we try . <end>', '<start> on essaye . <end>'],\n",
       " ['<start> we won . <end>', '<start> nous avons gagne . <end>'],\n",
       " ['<start> we won . <end>', '<start> nous gagnames . <end>'],\n",
       " ['<start> we won . <end>', '<start> nous l avons emporte . <end>'],\n",
       " ['<start> we won . <end>', '<start> nous l emportames . <end>'],\n",
       " ['<start> ask tom . <end>', '<start> demande a tom . <end>'],\n",
       " ['<start> awesome ! <end>', '<start> fantastique ! <end>'],\n",
       " ['<start> be calm . <end>', '<start> sois calme ! <end>'],\n",
       " ['<start> be calm . <end>', '<start> soyez calme ! <end>'],\n",
       " ['<start> be calm . <end>', '<start> soyez calmes ! <end>'],\n",
       " ['<start> be cool . <end>', '<start> sois detendu ! <end>'],\n",
       " ['<start> be fair . <end>', '<start> sois juste ! <end>'],\n",
       " ['<start> be fair . <end>', '<start> soyez juste ! <end>'],\n",
       " ['<start> be fair . <end>', '<start> soyez justes ! <end>'],\n",
       " ['<start> be fair . <end>', '<start> sois equitable ! <end>'],\n",
       " ['<start> be fair . <end>', '<start> soyez equitable ! <end>'],\n",
       " ['<start> be fair . <end>', '<start> soyez equitables ! <end>'],\n",
       " ['<start> be kind . <end>', '<start> sois gentil . <end>'],\n",
       " ['<start> be nice . <end>', '<start> sois gentil ! <end>'],\n",
       " ['<start> be nice . <end>', '<start> sois gentille ! <end>'],\n",
       " ['<start> be nice . <end>', '<start> soyez gentil ! <end>'],\n",
       " ['<start> be nice . <end>', '<start> soyez gentille ! <end>'],\n",
       " ['<start> be nice . <end>', '<start> soyez gentils ! <end>'],\n",
       " ['<start> be nice . <end>', '<start> soyez gentilles ! <end>'],\n",
       " ['<start> beat it . <end>', '<start> degage ! <end>'],\n",
       " ['<start> call me . <end>', '<start> appelle moi ! <end>'],\n",
       " ['<start> call me . <end>', '<start> appellez moi ! <end>'],\n",
       " ['<start> call us . <end>', '<start> appelle nous ! <end>'],\n",
       " ['<start> call us . <end>', '<start> appelez nous ! <end>'],\n",
       " ['<start> come in . <end>', '<start> entrez ! <end>'],\n",
       " ['<start> come in . <end>', '<start> entre . <end>'],\n",
       " ['<start> come in . <end>', '<start> entre ! <end>'],\n",
       " ['<start> come in . <end>', '<start> entrez ! <end>'],\n",
       " ['<start> come on ! <end>', '<start> allez ! <end>'],\n",
       " ['<start> come on . <end>', '<start> allez ! <end>']]"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_dataset(filepath, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<SOS> go . <EOS>', '<SOS> va ! <EOS>'],\n",
       " ['<SOS> hi . <EOS>', '<SOS> salut ! <EOS>'],\n",
       " ['<SOS> run ! <EOS>', '<SOS> cours ! <EOS>'],\n",
       " ['<SOS> run ! <EOS>', '<SOS> courez ! <EOS>'],\n",
       " ['<SOS> wow ! <EOS>', '<SOS> ca alors ! <EOS>'],\n",
       " ['<SOS> fire ! <EOS>', '<SOS> au feu ! <EOS>'],\n",
       " ['<SOS> help ! <EOS>', '<SOS> a l aide ! <EOS>'],\n",
       " ['<SOS> jump . <EOS>', '<SOS> saute . <EOS>'],\n",
       " ['<SOS> stop ! <EOS>', '<SOS> ca suffit ! <EOS>'],\n",
       " ['<SOS> stop ! <EOS>', '<SOS> stop ! <EOS>'],\n",
       " ['<SOS> stop ! <EOS>', '<SOS> arrete toi ! <EOS>'],\n",
       " ['<SOS> wait ! <EOS>', '<SOS> attends ! <EOS>'],\n",
       " ['<SOS> wait ! <EOS>', '<SOS> attendez ! <EOS>'],\n",
       " ['<SOS> go on . <EOS>', '<SOS> poursuis . <EOS>'],\n",
       " ['<SOS> go on . <EOS>', '<SOS> continuez . <EOS>'],\n",
       " ['<SOS> go on . <EOS>', '<SOS> poursuivez . <EOS>'],\n",
       " ['<SOS> hello ! <EOS>', '<SOS> bonjour ! <EOS>'],\n",
       " ['<SOS> hello ! <EOS>', '<SOS> salut ! <EOS>'],\n",
       " ['<SOS> i see . <EOS>', '<SOS> je comprends . <EOS>'],\n",
       " ['<SOS> i try . <EOS>', '<SOS> j essaye . <EOS>'],\n",
       " ['<SOS> i won ! <EOS>', '<SOS> j ai gagne ! <EOS>'],\n",
       " ['<SOS> i won ! <EOS>', '<SOS> je l ai emporte ! <EOS>'],\n",
       " ['<SOS> oh no ! <EOS>', '<SOS> oh non ! <EOS>'],\n",
       " ['<SOS> attack ! <EOS>', '<SOS> attaque ! <EOS>'],\n",
       " ['<SOS> attack ! <EOS>', '<SOS> attaquez ! <EOS>'],\n",
       " ['<SOS> cheers ! <EOS>', '<SOS> sante ! <EOS>'],\n",
       " ['<SOS> cheers ! <EOS>', '<SOS> a votre sante ! <EOS>'],\n",
       " ['<SOS> cheers ! <EOS>', '<SOS> merci ! <EOS>'],\n",
       " ['<SOS> cheers ! <EOS>', '<SOS> tchin tchin ! <EOS>'],\n",
       " ['<SOS> get up . <EOS>', '<SOS> leve toi . <EOS>'],\n",
       " ['<SOS> go now . <EOS>', '<SOS> va , maintenant . <EOS>'],\n",
       " ['<SOS> go now . <EOS>', '<SOS> allez y maintenant . <EOS>'],\n",
       " ['<SOS> go now . <EOS>', '<SOS> vas y maintenant . <EOS>'],\n",
       " ['<SOS> got it ! <EOS>', '<SOS> j ai pige ! <EOS>'],\n",
       " ['<SOS> got it ! <EOS>', '<SOS> compris ! <EOS>'],\n",
       " ['<SOS> got it ? <EOS>', '<SOS> pige ? <EOS>'],\n",
       " ['<SOS> got it ? <EOS>', '<SOS> compris ? <EOS>'],\n",
       " ['<SOS> got it ? <EOS>', '<SOS> t as capte ? <EOS>'],\n",
       " ['<SOS> hop in . <EOS>', '<SOS> monte . <EOS>'],\n",
       " ['<SOS> hop in . <EOS>', '<SOS> montez . <EOS>'],\n",
       " ['<SOS> hug me . <EOS>', '<SOS> serre moi dans tes bras ! <EOS>'],\n",
       " ['<SOS> hug me . <EOS>', '<SOS> serrez moi dans vos bras ! <EOS>'],\n",
       " ['<SOS> i fell . <EOS>', '<SOS> je suis tombee . <EOS>'],\n",
       " ['<SOS> i fell . <EOS>', '<SOS> je suis tombe . <EOS>'],\n",
       " ['<SOS> i know . <EOS>', '<SOS> je sais . <EOS>'],\n",
       " ['<SOS> i left . <EOS>', '<SOS> je suis parti . <EOS>'],\n",
       " ['<SOS> i left . <EOS>', '<SOS> je suis partie . <EOS>'],\n",
       " ['<SOS> i lost . <EOS>', '<SOS> j ai perdu . <EOS>'],\n",
       " ['<SOS> i m . <EOS>', '<SOS> j ai ans . <EOS>'],\n",
       " ['<SOS> i m ok . <EOS>', '<SOS> je vais bien . <EOS>'],\n",
       " ['<SOS> i m ok . <EOS>', '<SOS> ca va . <EOS>'],\n",
       " ['<SOS> listen . <EOS>', '<SOS> ecoutez ! <EOS>'],\n",
       " ['<SOS> no way ! <EOS>', '<SOS> c est pas possible ! <EOS>'],\n",
       " ['<SOS> no way ! <EOS>', '<SOS> impossible ! <EOS>'],\n",
       " ['<SOS> no way ! <EOS>', '<SOS> en aucun cas . <EOS>'],\n",
       " ['<SOS> no way ! <EOS>', '<SOS> sans facons ! <EOS>'],\n",
       " ['<SOS> no way ! <EOS>', '<SOS> c est hors de question ! <EOS>'],\n",
       " ['<SOS> no way ! <EOS>', '<SOS> il n en est pas question ! <EOS>'],\n",
       " ['<SOS> no way ! <EOS>', '<SOS> c est exclu ! <EOS>'],\n",
       " ['<SOS> no way ! <EOS>', '<SOS> en aucune maniere ! <EOS>'],\n",
       " ['<SOS> no way ! <EOS>', '<SOS> hors de question ! <EOS>'],\n",
       " ['<SOS> really ? <EOS>', '<SOS> vraiment ? <EOS>'],\n",
       " ['<SOS> really ? <EOS>', '<SOS> vrai ? <EOS>'],\n",
       " ['<SOS> really ? <EOS>', '<SOS> ah bon ? <EOS>'],\n",
       " ['<SOS> thanks . <EOS>', '<SOS> merci ! <EOS>'],\n",
       " ['<SOS> we try . <EOS>', '<SOS> on essaye . <EOS>'],\n",
       " ['<SOS> we won . <EOS>', '<SOS> nous avons gagne . <EOS>'],\n",
       " ['<SOS> we won . <EOS>', '<SOS> nous gagnames . <EOS>'],\n",
       " ['<SOS> we won . <EOS>', '<SOS> nous l avons emporte . <EOS>'],\n",
       " ['<SOS> we won . <EOS>', '<SOS> nous l emportames . <EOS>'],\n",
       " ['<SOS> ask tom . <EOS>', '<SOS> demande a tom . <EOS>'],\n",
       " ['<SOS> awesome ! <EOS>', '<SOS> fantastique ! <EOS>'],\n",
       " ['<SOS> be calm . <EOS>', '<SOS> sois calme ! <EOS>'],\n",
       " ['<SOS> be calm . <EOS>', '<SOS> soyez calme ! <EOS>'],\n",
       " ['<SOS> be calm . <EOS>', '<SOS> soyez calmes ! <EOS>'],\n",
       " ['<SOS> be cool . <EOS>', '<SOS> sois detendu ! <EOS>'],\n",
       " ['<SOS> be fair . <EOS>', '<SOS> sois juste ! <EOS>'],\n",
       " ['<SOS> be fair . <EOS>', '<SOS> soyez juste ! <EOS>'],\n",
       " ['<SOS> be fair . <EOS>', '<SOS> soyez justes ! <EOS>'],\n",
       " ['<SOS> be fair . <EOS>', '<SOS> sois equitable ! <EOS>'],\n",
       " ['<SOS> be fair . <EOS>', '<SOS> soyez equitable ! <EOS>'],\n",
       " ['<SOS> be fair . <EOS>', '<SOS> soyez equitables ! <EOS>'],\n",
       " ['<SOS> be kind . <EOS>', '<SOS> sois gentil . <EOS>'],\n",
       " ['<SOS> be nice . <EOS>', '<SOS> sois gentil ! <EOS>'],\n",
       " ['<SOS> be nice . <EOS>', '<SOS> sois gentille ! <EOS>'],\n",
       " ['<SOS> be nice . <EOS>', '<SOS> soyez gentil ! <EOS>'],\n",
       " ['<SOS> be nice . <EOS>', '<SOS> soyez gentille ! <EOS>'],\n",
       " ['<SOS> be nice . <EOS>', '<SOS> soyez gentils ! <EOS>'],\n",
       " ['<SOS> be nice . <EOS>', '<SOS> soyez gentilles ! <EOS>'],\n",
       " ['<SOS> beat it . <EOS>', '<SOS> degage ! <EOS>'],\n",
       " ['<SOS> call me . <EOS>', '<SOS> appelle moi ! <EOS>'],\n",
       " ['<SOS> call me . <EOS>', '<SOS> appellez moi ! <EOS>'],\n",
       " ['<SOS> call us . <EOS>', '<SOS> appelle nous ! <EOS>'],\n",
       " ['<SOS> call us . <EOS>', '<SOS> appelez nous ! <EOS>'],\n",
       " ['<SOS> come in . <EOS>', '<SOS> entrez ! <EOS>'],\n",
       " ['<SOS> come in . <EOS>', '<SOS> entre . <EOS>'],\n",
       " ['<SOS> come in . <EOS>', '<SOS> entre ! <EOS>'],\n",
       " ['<SOS> come in . <EOS>', '<SOS> entrez ! <EOS>'],\n",
       " ['<SOS> come on ! <EOS>', '<SOS> allez ! <EOS>'],\n",
       " ['<SOS> come on . <EOS>', '<SOS> allez ! <EOS>']]"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sent_pairs(filepath,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word_Index:\n",
    "    def __init__ (self, inp_lang):\n",
    "        self.inp_lang = inp_lang\n",
    "        self.vocab = set()\n",
    "        self.w_idx = {} #word to index\n",
    "        self.idx_w = {} #index to word\n",
    "        \n",
    "        self.lang_index_transform()\n",
    "    def lang_index_transform(self):\n",
    "        for line in self.inp_lang:\n",
    "            self.vocab.update(line.split(' '))\n",
    "        self.vocab = sorted(self.vocab)\n",
    "        self.w_idx['<pad>'] = 0\n",
    "        for idx,w in enumerate(self.vocab):\n",
    "            self.w_idx[w] = idx+1\n",
    "        for w, idx in self.w_idx.items():\n",
    "            self.idx_w[idx] = w\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<pad>': 0, '!': 1, ',': 2, '.': 3, '<EOS>': 4, '<SOS>': 5, '?': 6, 'a': 7, 'aboient': 8, 'accord': 9, 'agreable': 10, 'ah': 11, 'ai': 12, 'aide': 13, 'aidez': 14, 'ailleurs': 15, 'aller': 16, 'allez': 17, 'allons': 18, 'alors': 19, 'amuse': 20, 'amusez': 21, 'ans': 22, 'appelez': 23, 'appelle': 24, 'appellez': 25, 'apres': 26, 'arrete': 27, 'as': 28, 'asseyez': 29, 'assieds': 30, 'attaque': 31, 'attaquez': 32, 'attendez': 33, 'attends': 34, 'attention': 35, 'attrape': 36, 'attrapez': 37, 'au': 38, 'aucun': 39, 'aucune': 40, 'aussi': 41, 'avance': 42, 'avancez': 43, 'avertis': 44, 'avez': 45, 'avis': 46, 'avons': 47, 'battues': 48, 'battus': 49, 'beau': 50, 'bibi': 51, 'bien': 52, 'bientot': 53, 'bienvenue': 54, 'bizarre': 55, 'bon': 56, 'bonjour': 57, 'bouge': 58, 'boulot': 59, 'bras': 60, 'c': 61, 'ca': 62, 'calin': 63, 'calme': 64, 'calmes': 65, 'calmez': 66, 'camp': 67, 'capte': 68, 'cas': 69, 'casse': 70, 'ce': 71, 'ceci': 72, 'certain': 73, 'chauve': 74, 'chez': 75, 'chiens': 76, 'chouette': 77, 'comme': 78, 'comprends': 79, 'compris': 80, 'conduis': 81, 'confiance': 82, 'content': 83, 'continue': 84, 'continuez': 85, 'courage': 86, 'courez': 87, 'cours': 88, 'court': 89, 'd': 90, 'dans': 91, 'de': 92, 'defaites': 93, 'defaits': 94, 'defense': 95, 'degage': 96, 'demande': 97, 'demandez': 98, 'depeche': 99, 'des': 100, 'descendez': 101, 'descends': 102, 'detends': 103, 'detendu': 104, 'devant': 105, 'diable': 106, 'dis': 107, 'disparais': 108, 'disponible': 109, 'dites': 110, 'donc': 111, 'doucement': 112, 'du': 113, 'echoue': 114, 'ecoutez': 115, 'ecris': 116, 'ecrivez': 117, 'effort': 118, 'elle': 119, 'elles': 120, 'embrasse': 121, 'embrassez': 122, 'emploie': 123, 'employez': 124, 'emportames': 125, 'emporte': 126, 'en': 127, 'entre': 128, 'entrer': 129, 'entrez': 130, 'epouse': 131, 'epousez': 132, 'equitable': 133, 'equitables': 134, 'essaie': 135, 'essaierai': 136, 'essaies': 137, 'essayai': 138, 'essaye': 139, 'essayer': 140, 'essayez': 141, 'est': 142, 'ete': 143, 'excellent': 144, 'exclu': 145, 'excuse': 146, 'excusez': 147, 'facons': 148, 'faible': 149, 'faineant': 150, 'faineante': 151, 'faire': 152, 'fais': 153, 'fait': 154, 'faites': 155, 'fantastique': 156, 'ferme': 157, 'feu': 158, 'fica': 159, 'fini': 160, 'fonctionne': 161, 'forme': 162, 'formidable': 163, 'fort': 164, 'fous': 165, 'foutre': 166, 'froid': 167, 'fumes': 168, 'gagnames': 169, 'gagne': 170, 'gagnerent': 171, 'garde': 172, 'gardez': 173, 'genial': 174, 'gentil': 175, 'gentille': 176, 'gentilles': 177, 'gentils': 178, 'goute': 179, 'goutez': 180, 'grande': 181, 'gras': 182, 'gros': 183, 'grosse': 184, 'grouille': 185, 'homme': 186, 'hors': 187, 'ici': 188, 'idee': 189, 'il': 190, 'ils': 191, 'impossible': 192, 'informez': 193, 'irai': 194, 'irons': 195, 'j': 196, 'je': 197, 'joignez': 198, 'joli': 199, 'joue': 200, 'juste': 201, 'justes': 202, 'l': 203, 'la': 204, 'lache': 205, 'lachez': 206, 'laid': 207, 'laide': 208, 'laissa': 209, 'laisse': 210, 'laissez': 211, 'lave': 212, 'lavez': 213, 'le': 214, 'les': 215, 'leur': 216, 'leve': 217, 'libre': 218, 'm': 219, 'maintenant': 220, 'maison': 221, 'mal': 222, 'malade': 223, 'maniere': 224, 'marche': 225, 'marrant': 226, 'marrees': 227, 'marres': 228, 'me': 229, 'mecs': 230, 'meme': 231, 'menottez': 232, 'mens': 233, 'ment': 234, 'menti': 235, 'merci': 236, 'meurs': 237, 'mignon': 238, 'mince': 239, 'moi': 240, 'monte': 241, 'montez': 242, 'montre': 243, 'montrez': 244, 'mort': 245, 'morte': 246, 'mouille': 247, 'mouillee': 248, 'mourez': 249, 'muera': 250, 'n': 251, 'ne': 252, 'neuf': 253, 'non': 254, 'nous': 255, 'nouveau': 256, 'occupe': 257, 'occupee': 258, 'oh': 259, 'oiseaux': 260, 'on': 261, 'ont': 262, 'ordonne': 263, 'ordonnee': 264, 'oublie': 265, 'oubliez': 266, 'ouvre': 267, 'paierai': 268, 'paresseuse': 269, 'paresseux': 270, 'parfait': 271, 'parle': 272, 'parlez': 273, 'pars': 274, 'parti': 275, 'partie': 276, 'partir': 277, 'partit': 278, 'pas': 279, 'passe': 280, 'paye': 281, 'perdimes': 282, 'perdu': 283, 'peu': 284, 'pige': 285, 'pleure': 286, 'plus': 287, 'porte': 288, 'possible': 289, 'pour': 290, 'poursuis': 291, 'poursuivez': 292, 'pouvons': 293, 'prends': 294, 'prenez': 295, 'pressez': 296, 'previens': 297, 'profondeur': 298, 'puis': 299, 'qu': 300, 'quelle': 301, 'question': 302, 'qui': 303, 'quittez': 304, 'quoi': 305, 'rapide': 306, 'rassasie': 307, 'rattrape': 308, 'realiste': 309, 'recule': 310, 'reculez': 311, 'refuse': 312, 'regarde': 313, 'regardez': 314, 'rendre': 315, 'rentre': 316, 'rentrez': 317, 'repare': 318, 'reparez': 319, 'repondez': 320, 'repu': 321, 'reste': 322, 'restee': 323, 'retard': 324, 'retire': 325, 'retirez': 326, 'reveille': 327, 'reveillez': 328, 'revenez': 329, 'revenu': 330, 'reviens': 331, 'revoila': 332, 'revoyure': 333, 'riche': 334, 'rien': 335, 'rigolo': 336, 'rire': 337, 'rouge': 338, 'rouler': 339, 's': 340, 'sais': 341, 'salut': 342, 'sans': 343, 'sante': 344, 'saute': 345, 'sauve': 346, 'sauvez': 347, 'savait': 348, 'savons': 349, 'securite': 350, 'sens': 351, 'sentez': 352, 'serre': 353, 'serrez': 354, 'sers': 355, 'si': 356, 'sien': 357, 'sienne': 358, 'sois': 359, 'sommes': 360, 'sors': 361, 'sortez': 362, 'sorti': 363, 'sortie': 364, 'souhaits': 365, 'sourd': 366, 'sourde': 367, 'soyez': 368, 'stop': 369, 'stoppez': 370, 'suffit': 371, 'suis': 372, 'suivez': 373, 'sur': 374, 'sure': 375, 't': 376, 'tais': 377, 'taisez': 378, 'tard': 379, 'tchin': 380, 'te': 381, 'telephonai': 382, 'telephone': 383, 'tenez': 384, 'tente': 385, 'tes': 386, 'tiens': 387, 'timide': 388, 'toi': 389, 'tom': 390, 'tombe': 391, 'tombee': 392, 'tomber': 393, 'touche': 394, 'touchee': 395, 'touchez': 396, 'tout': 397, 'travail': 398, 'triste': 399, 'trop': 400, 'trouve': 401, 'trouvez': 402, 'tu': 403, 'un': 404, 'usage': 405, 'utilise': 406, 'utilisez': 407, 'va': 408, 'vais': 409, 'vas': 410, 'venez': 411, 'venu': 412, 'venue': 413, 'viens': 414, 'voir': 415, 'volent': 416, 'vos': 417, 'votre': 418, 'vous': 419, 'vrai': 420, 'vraiment': 421, 'vu': 422, 'y': 423}\n"
     ]
    }
   ],
   "source": [
    "sent_pairs = generate_sent_pairs(filepath, num_of_examples)\n",
    "# sent_pairs\n",
    "vocab = set()\n",
    "inp_lang = LanguageIndex(sp for en, sp in sent_pairs)\n",
    "# print(inp_lang.word2idx)\n",
    "inp_lanuage = Word_Index(sp for en, sp in sent_pairs)\n",
    "print(inp_lanuage.w_idx)\n",
    "for first, second in sent_pairs:\n",
    "    \n",
    "    enc_inp_lang = LanguageIndex(second)\n",
    "#print(first)\n",
    "# print(enc_inp_lang.word2idx)\n",
    "for line in first:\n",
    "#     print(line)\n",
    "    vocab.update(line)\n",
    "vocab = sorted(vocab)\n",
    "# print(vocab)\n",
    "for first, second in sent_pairs:\n",
    "    dec_target_lang = Word_Index(first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '.', '<', '>', 'E', 'O', 'S', 'a', 'i', 'm', 'o', 'r', 's', 'y']\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'<SOS>'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-254-16629531ba96>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlang2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc_inp_lang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0menc_inp_lang_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '<SOS>'"
     ]
    }
   ],
   "source": [
    "sent_pairs = generate_sent_pairs(filepath, num_of_examples)\n",
    "    #create language indexes to create tensors\n",
    "for first, second in sent_pairs:\n",
    "    enc_inp_lang = Word_Index(second)\n",
    "for first, second in sent_pairs:\n",
    "    dec_target_lang = Word_Index(first)\n",
    "print(dec_target_lang.vocab)\n",
    "enc_inp_lang_tensor = []\n",
    "for lang1, lang2 in sent_pairs:\n",
    "    tensor =[]\n",
    "    for w in lang2.split(' '):\n",
    "        tensor.append(enc_inp_lang.w_idx[w])\n",
    "    enc_inp_lang_tensor.append(tensor)\n",
    "    \n",
    "dec_target_lang_tensor = []\n",
    "for lang1, lang2 in sent_pairs:\n",
    "    tensor =[]\n",
    "    for w in lang1.split(' '):\n",
    "        tensor.append(dec_target_lang.w_idx[w]) \n",
    "    dec_target_lang_tensor.append(tensor) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tensors(filepath, num_of_examples):\n",
    "    sent_pairs = generate_sent_pairs(filepath, num_of_examples)\n",
    "    #create language indexes to create tensors\n",
    "    enc_inp_lang = Word_Index(second for first, second in sent_pairs)\n",
    "    dec_target_lang = Word_Index(first for first, second in sent_pairs)\n",
    "    #create tensors, find_max_len tensor to pad all the inputs\n",
    "    enc_inp_lang_tensor = []\n",
    "    for lang1, lang2 in sent_pairs:\n",
    "        tensor =[]\n",
    "        for w in lang2.split(' '):\n",
    "            tensor.append(enc_inp_lang.w_idx[w])\n",
    "        enc_inp_lang_tensor.append(tensor)\n",
    "    print(enc_inp_lang_tensor)\n",
    "    dec_target_lang_tensor = []\n",
    "    for lang1, lang2 in sent_pairs:\n",
    "        tensor =[]\n",
    "        for w in lang1.split(' '):\n",
    "             tensor.append(dec_target_lang.w_idx[w]) \n",
    "        dec_target_lang_tensor.append(tensor) \n",
    "    \n",
    "    enc_inp_max_len = max(len(tensor) for tensor in enc_inp_lang_tensor)\n",
    "        \n",
    "    dec_target_max_len = max(len(tensor) for tensor in dec_target_lang_tensor)\n",
    "    #padding input tensors at encoder and target sensors at decoder\n",
    "    \n",
    "    enc_inp_lang_tensor = tf.keras.preprocessing.sequence.pad_sequences(enc_inp_lang_tensor, enc_inp_max_len, padding = 'post')\n",
    "    \n",
    "    dec_target_lang_tensor = tf.keras.preprocessing.sequence.pad_sequences(dec_target_lang_tensor, dec_target_max_len, padding = 'post')\n",
    "    \n",
    "    return enc_inp_lang_tensor, dec_target_lang_tensor, enc_inp_lang, dec_target_lang , enc_inp_max_len, dec_target_max_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5, 408, 1, 4], [5, 342, 1, 4], [5, 88, 1, 4], [5, 87, 1, 4], [5, 62, 19, 1, 4], [5, 38, 158, 1, 4], [5, 7, 203, 13, 1, 4], [5, 345, 3, 4], [5, 62, 371, 1, 4], [5, 369, 1, 4], [5, 27, 389, 1, 4], [5, 34, 1, 4], [5, 33, 1, 4], [5, 291, 3, 4], [5, 85, 3, 4], [5, 292, 3, 4], [5, 57, 1, 4], [5, 342, 1, 4], [5, 197, 79, 3, 4], [5, 196, 139, 3, 4], [5, 196, 12, 170, 1, 4], [5, 197, 203, 12, 126, 1, 4], [5, 259, 254, 1, 4], [5, 31, 1, 4], [5, 32, 1, 4], [5, 344, 1, 4], [5, 7, 418, 344, 1, 4], [5, 236, 1, 4], [5, 380, 380, 1, 4], [5, 217, 389, 3, 4], [5, 408, 2, 220, 3, 4], [5, 17, 423, 220, 3, 4], [5, 410, 423, 220, 3, 4], [5, 196, 12, 285, 1, 4], [5, 80, 1, 4], [5, 285, 6, 4], [5, 80, 6, 4], [5, 376, 28, 68, 6, 4], [5, 241, 3, 4], [5, 242, 3, 4], [5, 353, 240, 91, 386, 60, 1, 4], [5, 354, 240, 91, 417, 60, 1, 4], [5, 197, 372, 392, 3, 4], [5, 197, 372, 391, 3, 4], [5, 197, 341, 3, 4], [5, 197, 372, 275, 3, 4], [5, 197, 372, 276, 3, 4], [5, 196, 12, 283, 3, 4], [5, 196, 12, 22, 3, 4], [5, 197, 409, 52, 3, 4], [5, 62, 408, 3, 4], [5, 115, 1, 4], [5, 61, 142, 279, 289, 1, 4], [5, 192, 1, 4], [5, 127, 39, 69, 3, 4], [5, 343, 148, 1, 4], [5, 61, 142, 187, 92, 302, 1, 4], [5, 190, 251, 127, 142, 279, 302, 1, 4], [5, 61, 142, 145, 1, 4], [5, 127, 40, 224, 1, 4], [5, 187, 92, 302, 1, 4], [5, 421, 6, 4], [5, 420, 6, 4], [5, 11, 56, 6, 4], [5, 236, 1, 4], [5, 261, 139, 3, 4], [5, 255, 47, 170, 3, 4], [5, 255, 169, 3, 4], [5, 255, 203, 47, 126, 3, 4], [5, 255, 203, 125, 3, 4], [5, 97, 7, 390, 3, 4], [5, 156, 1, 4], [5, 359, 64, 1, 4], [5, 368, 64, 1, 4], [5, 368, 65, 1, 4], [5, 359, 104, 1, 4], [5, 359, 201, 1, 4], [5, 368, 201, 1, 4], [5, 368, 202, 1, 4], [5, 359, 133, 1, 4], [5, 368, 133, 1, 4], [5, 368, 134, 1, 4], [5, 359, 175, 3, 4], [5, 359, 175, 1, 4], [5, 359, 176, 1, 4], [5, 368, 175, 1, 4], [5, 368, 176, 1, 4], [5, 368, 178, 1, 4], [5, 368, 177, 1, 4], [5, 96, 1, 4], [5, 24, 240, 1, 4], [5, 25, 240, 1, 4], [5, 24, 255, 1, 4], [5, 23, 255, 1, 4], [5, 130, 1, 4], [5, 128, 3, 4], [5, 128, 1, 4], [5, 130, 1, 4], [5, 17, 1, 4], [5, 17, 1, 4], [5, 414, 1, 4], [5, 411, 1, 4], [5, 210, 393, 1, 4], [5, 211, 393, 1, 4], [5, 210, 214, 393, 1, 4], [5, 211, 214, 393, 1, 4], [5, 362, 1, 4], [5, 361, 1, 4], [5, 362, 1, 4], [5, 361, 3, 4], [5, 70, 389, 3, 4], [5, 96, 1, 4], [5, 274, 1, 4], [5, 408, 381, 152, 166, 1, 4], [5, 274, 1, 4], [5, 96, 1, 4], [5, 165, 214, 67, 1, 4], [5, 274, 90, 188, 3, 4], [5, 408, 376, 127, 1, 4], [5, 108, 1, 4], [5, 17, 419, 127, 1, 4], [5, 317, 7, 204, 221, 3, 4], [5, 316, 7, 204, 221, 3, 4], [5, 316, 75, 389, 3, 4], [5, 317, 75, 419, 3, 4], [5, 408, 112, 1, 4], [5, 17, 112, 1, 4], [5, 7, 204, 333, 3, 4], [5, 34, 404, 284, 1, 4], [5, 33, 404, 284, 1, 4], [5, 387, 56, 1, 4], [5, 384, 56, 1, 4], [5, 190, 209, 393, 3, 4], [5, 190, 7, 210, 393, 3, 4], [5, 190, 89, 3, 4], [5, 13, 240, 1, 4], [5, 13, 240, 3, 4], [5, 14, 240, 3, 4], [5, 14, 255, 1, 4], [5, 13, 255, 1, 4], [5, 252, 58, 287, 1, 4], [5, 252, 304, 279, 3, 4], [5, 153, 404, 63, 7, 390, 3, 4], [5, 197, 372, 113, 231, 46, 3, 4], [5, 197, 81, 3, 4], [5, 196, 138, 3, 4], [5, 196, 12, 139, 3, 4], [5, 196, 12, 385, 3, 4], [5, 196, 194, 3, 4], [5, 197, 372, 390, 3, 4], [5, 197, 372, 182, 3, 4], [5, 197, 372, 183, 3, 4], [5, 197, 372, 127, 162, 3, 4], [5, 197, 372, 394, 1, 4], [5, 197, 372, 395, 1, 4], [5, 197, 372, 223, 3, 4], [5, 197, 372, 399, 3, 4], [5, 197, 372, 388, 3, 4], [5, 197, 372, 247, 3, 4], [5, 197, 372, 248, 3, 4], [5, 61, 142, 51, 1, 4], [5, 198, 419, 3, 4], [5, 198, 419, 7, 255, 3, 4], [5, 172, 214, 1, 4], [5, 173, 214, 1, 4], [5, 121, 240, 3, 4], [5, 122, 240, 3, 4], [5, 240, 41, 3, 4], [5, 267, 240, 1, 4], [5, 267, 3, 4], [5, 271, 1, 4], [5, 7, 287, 3, 4], [5, 243, 240, 1, 4], [5, 244, 240, 1, 4], [5, 378, 419, 1, 4], [5, 157, 204, 1, 4], [5, 377, 389, 1, 4], [5, 157, 204, 1, 4], [5, 204, 157, 1, 4], [5, 7, 287, 379, 1, 4], [5, 294, 214, 1, 4], [5, 295, 214, 1, 4], [5, 107, 240, 1, 4], [5, 110, 240, 1, 4], [5, 390, 7, 170, 3, 4], [5, 327, 389, 1, 4], [5, 327, 389, 1, 4], [5, 328, 419, 1, 4], [5, 327, 389, 1, 4], [5, 328, 419, 1, 4], [5, 212, 389, 1, 4], [5, 213, 419, 1, 4], [5, 255, 349, 3, 4], [5, 255, 282, 3, 4], [5, 255, 47, 283, 3, 4], [5, 255, 168, 49, 3, 4], [5, 255, 168, 48, 3, 4], [5, 255, 168, 94, 3, 4], [5, 255, 168, 93, 3, 4], [5, 255, 47, 143, 94, 3, 4], [5, 255, 47, 143, 93, 3, 4], [5, 255, 47, 143, 49, 3, 4], [5, 255, 47, 143, 48, 3, 4], [5, 54, 1, 4], [5, 303, 7, 170, 6, 4], [5, 303, 203, 7, 126, 6, 4], [5, 403, 88, 3, 4], [5, 372, 197, 183, 6, 4], [5, 372, 197, 184, 6, 4], [5, 97, 216, 3, 4], [5, 98, 216, 3, 4], [5, 310, 1, 4], [5, 311, 3, 4], [5, 310, 1, 4], [5, 311, 3, 4], [5, 325, 389, 1, 4], [5, 326, 419, 3, 4], [5, 359, 404, 186, 1, 4], [5, 368, 404, 186, 1, 4], [5, 359, 64, 1, 4], [5, 368, 64, 1, 4], [5, 368, 65, 1, 4], [5, 40, 189, 3, 4], [5, 196, 127, 341, 166, 335, 3, 4], [5, 24, 390, 3, 4], [5, 23, 390, 3, 4], [5, 86, 1, 4], [5, 103, 389, 1, 4], [5, 232, 214, 3, 4], [5, 42, 1, 4], [5, 43, 1, 4], [5, 84, 7, 339, 1, 4], [5, 85, 7, 339, 1, 4], [5, 401, 390, 3, 4], [5, 402, 390, 3, 4], [5, 319, 72, 3, 4], [5, 318, 62, 3, 4], [5, 205, 389, 1, 4], [5, 102, 1, 4], [5, 101, 1, 4], [5, 205, 389, 1, 4], [5, 206, 419, 1, 4], [5, 408, 415, 15, 356, 196, 423, 372, 1, 4], [5, 96, 1, 4], [5, 408, 38, 106, 1, 4], [5, 359, 309, 1, 4], [5, 410, 423, 3, 4], [5, 291, 1, 4], [5, 280, 105, 1, 4], [5, 410, 423, 1, 4], [5, 52, 200, 1, 4], [5, 56, 59, 1, 4], [5, 50, 398, 1, 4], [5, 36, 214, 3, 4], [5, 37, 214, 3, 4], [5, 20, 389, 52, 1, 4], [5, 21, 419, 52, 1, 4], [5, 190, 139, 3, 4], [5, 190, 142, 247, 3, 4], [5, 13, 390, 3, 4], [5, 14, 390, 3, 4], [5, 342, 2, 215, 230, 1, 4], [5, 78, 61, 142, 238, 1, 4], [5, 301, 298, 6, 4], [5, 78, 61, 142, 77, 1, 4], [5, 78, 61, 142, 175, 1, 4], [5, 61, 142, 113, 199, 1, 4], [5, 78, 61, 142, 10, 1, 4], [5, 153, 240, 337, 3, 4], [5, 99, 389, 3, 4], [5, 185, 1, 4], [5, 296, 419, 1, 4], [5, 159, 1, 4], [5, 197, 372, 182, 3, 4], [5, 197, 219, 127, 372, 52, 363, 3, 4], [5, 197, 219, 127, 372, 52, 364, 3, 4], [5, 197, 203, 12, 154, 3, 4], [5, 61, 142, 240, 303, 203, 12, 154, 3, 4], [5, 196, 12, 114, 3, 4], [5, 196, 12, 265, 3, 4], [5, 196, 12, 80, 3, 4], [5, 196, 12, 80, 3, 4], [5, 196, 12, 68, 3, 4], [5, 197, 382, 3, 4], [5, 196, 12, 383, 3, 4], [5, 197, 312, 3, 4], [5, 197, 214, 312, 3, 4], [5, 197, 203, 12, 422, 3, 4], [5, 197, 203, 12, 422, 3, 4], [5, 197, 372, 322, 3, 4], [5, 197, 372, 323, 3, 4], [5, 197, 203, 406, 3, 4], [5, 196, 127, 153, 405, 3, 4], [5, 197, 219, 127, 355, 3, 4], [5, 197, 268, 3, 4], [5, 197, 409, 140, 3, 4], [5, 196, 136, 3, 4], [5, 197, 372, 330, 3, 4], [5, 229, 332, 3, 4], [5, 197, 372, 74, 3, 4], [5, 197, 372, 257, 3, 4], [5, 197, 372, 258, 3, 4], [5, 197, 372, 64, 3, 4], [5, 196, 12, 167, 3, 4], [5, 197, 372, 366, 3, 4], [5, 197, 372, 367, 3, 4], [5, 196, 127, 12, 160, 3, 4], [5, 197, 372, 306, 3, 4], [5, 397, 408, 52, 3, 4], [5, 197, 409, 52, 3, 4], [5, 62, 408, 3, 4], [5, 197, 372, 218, 1, 4], [5, 197, 372, 218, 3, 4], [5, 197, 372, 109, 3, 4], [5, 197, 372, 321, 1, 4], [5, 197, 372, 307, 1, 4], [5, 197, 372, 83, 3, 4], [5, 197, 372, 75, 240, 3, 4], [5, 197, 372, 127, 324, 3, 4], [5, 197, 372, 270, 3, 4], [5, 197, 372, 150, 3, 4], [5, 197, 372, 269, 3, 4], [5, 197, 372, 151, 3, 4], [5, 197, 409, 52, 3, 4], [5, 197, 229, 288, 52, 3, 4], [5, 197, 372, 334, 3, 4], [5, 197, 372, 127, 350, 3, 4], [5, 197, 372, 223, 3, 4], [5, 196, 127, 372, 73, 3, 4], [5, 197, 372, 73, 3, 4], [5, 196, 127, 372, 374, 3, 4], [5, 196, 127, 372, 375, 3, 4], [5, 197, 372, 181, 3, 4], [5, 197, 372, 239, 3, 4], [5, 197, 372, 263, 3, 4], [5, 197, 372, 264, 3, 4], [5, 197, 372, 207, 3, 4], [5, 197, 372, 208, 3, 4], [5, 197, 372, 149, 3, 4], [5, 197, 409, 52, 3, 4], [5, 197, 229, 288, 52, 3, 4], [5, 196, 12, 170, 3, 4], [5, 197, 203, 12, 126, 3, 4], [5, 62, 13, 3, 4], [5, 62, 154, 222, 3, 4], [5, 119, 225, 3, 4], [5, 62, 161, 3, 4], [5, 61, 142, 390, 3, 4], [5, 61, 142, 226, 3, 4], [5, 61, 142, 336, 3, 4], [5, 61, 142, 214, 357, 3, 4], [5, 61, 142, 204, 358, 3, 4], [5, 61, 142, 256, 3, 4], [5, 61, 142, 253, 3, 4], [5, 61, 142, 55, 3, 4], [5, 190, 142, 338, 3, 4], [5, 61, 142, 399, 3, 4], [5, 95, 90, 129, 3, 4], [5, 251, 130, 279, 3, 4], [5, 121, 390, 3, 4], [5, 210, 393, 1, 4], [5, 211, 393, 1, 4], [5, 211, 240, 1, 4], [5, 210, 255, 1, 4], [5, 211, 255, 1, 4], [5, 18, 423, 1, 4], [5, 18, 423, 1, 4], [5, 35, 1, 4], [5, 313, 111, 1, 4], [5, 131, 240, 1, 4], [5, 132, 240, 1, 4], [5, 299, 197, 277, 6, 4], [5, 299, 197, 423, 16, 6, 4], [5, 299, 197, 219, 423, 315, 6, 4], [5, 346, 390, 3, 4], [5, 347, 390, 3, 4], [5, 119, 142, 413, 3, 4], [5, 119, 142, 246, 3, 4], [5, 119, 89, 3, 4], [5, 30, 389, 1, 4], [5, 29, 419, 1, 4], [5, 30, 389, 3, 4], [5, 30, 389, 188, 3, 4], [5, 29, 419, 188, 3, 4], [5, 272, 287, 164, 1, 4], [5, 273, 287, 164, 1, 4], [5, 27, 390, 3, 4], [5, 370, 390, 3, 4], [5, 179, 214, 3, 4], [5, 179, 204, 3, 4], [5, 180, 214, 3, 4], [5, 180, 204, 3, 4], [5, 107, 214, 7, 390, 3, 4], [5, 193, 127, 390, 3, 4], [5, 174, 1, 4], [5, 144, 1, 4], [5, 163, 1, 4], [5, 191, 171, 3, 4], [5, 120, 171, 3, 4], [5, 191, 262, 170, 3, 4], [5, 120, 262, 170, 3, 4], [5, 390, 142, 412, 3, 4], [5, 390, 142, 245, 3, 4], [5, 390, 250, 3, 4], [5, 390, 348, 3, 4], [5, 390, 142, 275, 3, 4], [5, 390, 278, 3, 4], [5, 390, 7, 235, 3, 4], [5, 390, 234, 3, 4], [5, 390, 7, 283, 3, 4], [5, 390, 7, 281, 3, 4], [5, 400, 379, 3, 4], [5, 155, 240, 82, 3, 4], [5, 153, 240, 82, 3, 4], [5, 153, 404, 118, 3, 4], [5, 137, 127, 1, 4], [5, 141, 127, 1, 4], [5, 135, 72, 1, 4], [5, 141, 72, 1, 4], [5, 406, 72, 3, 4], [5, 407, 72, 3, 4], [5, 123, 72, 1, 4], [5, 124, 72, 1, 4], [5, 44, 390, 3, 4], [5, 297, 390, 3, 4], [5, 313, 240, 1, 4], [5, 314, 240, 1, 4], [5, 314, 255, 1, 4], [5, 313, 255, 1, 4], [5, 255, 360, 90, 9, 3, 4], [5, 255, 195, 3, 4], [5, 290, 305, 152, 6, 4], [5, 7, 305, 56, 6, 4], [5, 300, 142, 71, 300, 261, 340, 142, 228, 1, 4], [5, 300, 142, 71, 300, 261, 340, 142, 227, 1, 4], [5, 303, 142, 412, 6, 4], [5, 303, 142, 245, 6, 4], [5, 303, 142, 391, 6, 4], [5, 303, 142, 190, 6, 4], [5, 116, 240, 1, 4], [5, 117, 240, 1, 4], [5, 403, 28, 283, 3, 4], [5, 419, 45, 283, 3, 4], [5, 26, 419, 3, 4], [5, 127, 200, 1, 158, 1, 4], [5, 372, 197, 127, 324, 6, 4], [5, 320, 240, 3, 4], [5, 30, 389, 1, 4], [5, 29, 419, 1, 4], [5, 215, 260, 416, 3, 4], [5, 7, 386, 365, 1, 4], [5, 24, 7, 204, 221, 1, 4], [5, 66, 419, 1, 4], [5, 64, 389, 3, 4], [5, 293, 255, 277, 6, 4], [5, 293, 255, 255, 127, 16, 6, 4], [5, 293, 255, 423, 16, 6, 4], [5, 36, 390, 3, 4], [5, 37, 390, 3, 4], [5, 308, 214, 3, 4], [5, 331, 1, 4], [5, 329, 1, 4], [5, 414, 188, 3, 4], [5, 411, 204, 3, 4], [5, 414, 1, 4], [5, 411, 1, 4], [5, 411, 188, 1, 4], [5, 414, 75, 255, 1, 4], [5, 411, 75, 255, 1, 4], [5, 414, 75, 240, 1, 4], [5, 411, 75, 240, 1, 4], [5, 414, 53, 1, 4], [5, 411, 53, 1, 4], [5, 66, 419, 1, 4], [5, 12, 197, 170, 6, 4], [5, 203, 12, 197, 126, 6, 4], [5, 142, 71, 240, 303, 12, 170, 6, 4], [5, 155, 214, 220, 3, 4], [5, 100, 76, 8, 3, 4], [5, 215, 76, 8, 3, 4], [5, 252, 97, 279, 1, 4], [5, 252, 286, 279, 1, 4], [5, 252, 237, 279, 1, 4], [5, 252, 249, 279, 1, 4], [5, 252, 233, 279, 3, 4], [5, 252, 87, 279, 3, 4], [5, 252, 88, 279, 3, 4], [5, 146, 240, 3, 4], [5, 147, 240, 3, 4], [5, 156, 1, 4], [5, 351, 62, 1, 4], [5, 352, 62, 1, 4], [5, 394, 62, 1, 4], [5, 396, 62, 1, 4], [5, 372, 240, 3, 4], [5, 372, 255, 1, 4], [5, 373, 255, 1, 4], [5, 265, 1, 4], [5, 265, 214, 1, 4], [5, 266, 1, 4]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(400, 400, 100, 100)"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_of_examples = 500\n",
    " \n",
    "enc_inp_lang_tensor, dec_target_lang_tensor, enc_inp_lang, dec_target_lang , enc_inp_max_len, dec_target_max_len = create_tensors(filepath, num_of_examples)\n",
    "\n",
    "#train-test splits\n",
    "train_inp, val_inp, train_targ, val_targ = train_test_split(enc_inp_lang_tensor, dec_target_lang_tensor, test_size = 0.2)\n",
    "len(train_inp), len(train_targ), len(val_inp), len(val_targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: ((64, 11), (64, 6)), types: (tf.int32, tf.int32)>\n"
     ]
    }
   ],
   "source": [
    "BUFFER_SIZE = len(train_inp)\n",
    "BATCH_SIZE = 64\n",
    "N_BATCH = BUFFER_SIZE//BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_inp_size = len(enc_inp_lang.w_idx)\n",
    "vocab_tar_size = len(dec_target_lang.w_idx)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((train_inp, train_targ)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gru(units):\n",
    "#   If you have a GPU, we recommend using CuDNNGRU(provides a 3x speedup than GRU)\n",
    "#   the code automatically does that.\n",
    "    if tf.test.is_gpu_available():\n",
    "        return tf.keras.layers.CuDNNGRU(units, \n",
    "                                    return_sequences=True, \n",
    "                                    return_state=True, \n",
    "                                    recurrent_initializer='glorot_uniform')\n",
    "    else:\n",
    "        return tf.keras.layers.GRU(units, \n",
    "                               return_sequences=True, \n",
    "                               return_state=True, \n",
    "                               recurrent_activation='sigmoid', \n",
    "                               recurrent_initializer='glorot_uniform')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, encoder_units, batch_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.encoder_units = encoder_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = gru(self.encoder_units)\n",
    "     #forcing child class to override parent's methods   \n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state = hidden)        \n",
    "        return output, state\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_size, self.encoder_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, decoder_units, batch_size):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.decoder_units = decoder_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = gru(self.decoder_units)\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "        # used for attention\n",
    "        self.W1 = tf.keras.layers.Dense(self.decoder_units)\n",
    "        self.W2 = tf.keras.layers.Dense(self.decoder_units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "     \n",
    "    #forcing child class to override parent's methods\n",
    "    def call(self, x, hidden, enc_output):\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        \n",
    "        # hidden shape == (batch_size, hidden size)\n",
    "        # hidden_expand shape == (batch_size, 1, hidden size)\n",
    "        # we are doing this to perform addition to calculate the score\n",
    "        hidden_expand = tf.expand_dims(hidden, 1)\n",
    "        \n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying tanh(FC(EO) + FC(H)) to self.V\n",
    "        score = self.V(tf.nn.tanh(self.W1(enc_output) + self.W2(hidden_expand)))\n",
    "        \n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        \n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * enc_output\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        print(context_vector)\n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        \n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "        \n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        \n",
    "        # output shape == (batch_size * 1, vocab)\n",
    "        x = self.fc(output)\n",
    "        \n",
    "        return x, state, attention_weights\n",
    "        \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_size, self.decoder_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer()\n",
    "\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = 1 - np.equal(real, 0)\n",
    "    loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred) * mask\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderRNN(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "decoder = AttnDecoderRNN(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "When subclassing the `Model` class, you should implement a `call` method.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-403-9a59006bd694>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0menc_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mdec_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menc_hidden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/newenv/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    755\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0min_deferred_mode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_call\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_call\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/newenv/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    833\u001b[0m     \"\"\"\n\u001b[1;32m    834\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_graph_network\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 835\u001b[0;31m       raise NotImplementedError('When subclassing the `Model` class, you should'\n\u001b[0m\u001b[1;32m    836\u001b[0m                                 ' implement a `call` method.')\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: When subclassing the `Model` class, you should implement a `call` method."
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    \n",
    "    hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for (batch, (inp, targ)) in enumerate(dataset):\n",
    "        loss = 0\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            enc_output, enc_hidden = encoder(inp, hidden)\n",
    "            \n",
    "            dec_hidden = enc_hidden\n",
    "            \n",
    "            dec_input = tf.expand_dims([dec_target_lang.w_idx['<SOS>']] * BATCH_SIZE, 1)       \n",
    "            \n",
    "            # Teacher forcing - feeding the target as the next input\n",
    "            for t in range(1, targ.shape[1]):\n",
    "                # passing enc_output to the decoder\n",
    "                predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "                \n",
    "                loss += loss_function(targ[:, t], predictions)\n",
    "                \n",
    "                # using teacher forcing\n",
    "                dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "        \n",
    "        batch_loss = (loss / int(targ.shape[1]))\n",
    "        \n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        variables = encoder.variables + decoder.variables\n",
    "        \n",
    "        gradients = tape.gradient(loss, variables)\n",
    "        \n",
    "        optimizer.apply_gradients(zip(gradients, variables))\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                         batch,\n",
    "                                                         batch_loss.numpy()))\n",
    "    # saving (checkpoint) the model every 2 epochs\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "    \n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                        total_loss / N_BATCH))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ):\n",
    "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "    \n",
    "    sentence = sent_preprocessing(sentence)\n",
    "\n",
    "    inputs = [enc_inp_lang.w_idx[i] for i in sentence.split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=max_length_inp, padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    \n",
    "    result = ''\n",
    "\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([dec_target_lang.w_idx['<SOS>']], 0)\n",
    "\n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
    "        \n",
    "        # storing the attention weigths to plot later on\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        result += dec_target_lang.idx_w[predicted_id] + ' '\n",
    "\n",
    "        if dec_target_lang.idx_w[predicted_id] == '<EOS>':\n",
    "            return result, sentence, attention_plot\n",
    "        \n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for plotting the attention weights\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "    \n",
    "    fontdict = {'fontsize': 14}\n",
    "    \n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ):\n",
    "    result, sentence, attention_plot = evaluate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)\n",
    "        \n",
    "    print('Input: {}'.format(sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "    \n",
    "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "    plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(u'appelez', encoder, decoder, enc_inp_lang, dec_target_lang, enc_inp_max_len, dec_target_max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                                                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
